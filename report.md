# Отчет

## 1. Название задачи
Перевод технических текстов с английского языка на русский

## 2. Описание подхода
Для оценки переводов были использованы три модели: Gemini, Groq и Cohere. Переводы анализировались по трём критериям: точность передачи терминов, естественность языка и сохранение смысла. Итоговый балл рассчитывался как среднее по этим трём показателям.

## 3. Таблица сравнения моделей

| Модель | Корректность перевода терминологии  | Читаемость   | Смысл | Итого |
|--------|-----------------------------------  |------------  |-------|-------|
| Cohere | 4.3                                 | 4.4          |  4.7   | 4.469|
| Gemini | 4.4                                 | 3.9          |  4.8   | 4.367|
| Groq   | 4.1                                 | 3.3          |  4.8   | 4.068| 


## 4. Примеры ответов моделей

**Пример 1 — исходный текст:**  
_"Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters."_

- **Gemini:** "Обучение глубоких нейронных сетей осложняется тем фактом, что распределение входных данных каждого слоя изменяется во время обучения, по мере изменения параметров предыдущих слоев. Это замедляет обучение, требуя более низких скоростей обучения и тщательной инициализации параметров, и делает обучение моделей с насыщающими нелинейностями заведомо сложным. Мы называем это явление внутренним ковариантным сдвигом и решаем проблему путем нормализации входных данных слоя. Наш метод черпает свою силу из включения нормализации в архитектуру модели и выполнения нормализации для каждого обучающего мини-пакета. Пакетная нормализация позволяет нам использовать гораздо более высокие скорости обучения и быть менее осторожными в отношении инициализации. Она также действует как регуляризатор, в некоторых случаях устраняя необходимость в Dropout. Примененная к передовой модели классификации изображений, пакетная нормализация достигает той же точности при в 14 раз меньшем количестве шагов обучения и превосходит исходную модель со значительным отрывом. Используя ансамбль пакетно-нормализованных сетей, мы улучшаем лучший опубликованный результат по классификации ImageNet: достигая 4,9% ошибки валидации top-5 (и 4,8% тестовой ошибки), превышая точность человеческих оценщиков."
- **Groq:** "Обучение глубоких нейронных сетей усложняется тем, что распределение входных данных каждого слоя меняется во время обучения, поскольку меняются параметры предыдущих слоев. Это замедляет обучение, требуя более низких скоростей обучения и тщательной инициализации параметров, и делает его чрезвычайно трудным обучать модели с насыщающими нелинейностями. Мы называем это явление внутренним сдвигом ковариант, и решаем эту проблему, нормализуя входные данные слоя. Наш метод черпает свою силу из того, что делает нормализацию частью архитектуры модели и выполняет нормализацию для каждой обучающей мини-партии. Пакетная нормализация позволяет нам использовать намного более высокие скорости обучения и быть менее осторожными об инициализации. Она также действует как регуляризатор, в некоторых случаях устраняя необходимость в Dropout. Примененная к моделью классификации изображений высшего уровня, пакетная нормализация достигает той же точности с 14-кратным уменьшением количества обучающих шагов и значительно превосходит исходную модель. Используя ансамбль пакетно-нормализованных сетей, мы улучшаем лучший опубликованный результат по классификации ImageNet: достигаем 4,9% ошибки проверки топ-5 (и 4,8% ошибки тестирования), превышая точность человеческих оценщиков."
- **Cohere:** "Обучение глубоких нейронных сетей усложняется тем фактом, что распределение входных данных каждого слоя изменяется во время обучения, поскольку параметры предыдущих слоев меняются. Это замедляет обучение, требуя более низких скоростей обучения и тщательной инициализации параметров, и делает крайне сложным обучение моделей с насыщающими нелинейностями. Мы называем это явление внутренним сдвигом ковариаты и решаем проблему путем нормализации входных данных слоя. Наш метод черпает свою силу из того, что делает нормализацию частью архитектуры модели и выполняет нормализацию для каждого мини-пакета обучения. Батч-нормализация позволяет нам использовать гораздо более высокие скорости обучения и быть менее осторожными в инициализации. Она также действует как регуляризатор, в некоторых случаях устраняя необходимость в Dropout. Примененная к современной модели классификации изображений, батч-нормализация достигает такой же точности с 14 раз меньшим количеством шагов обучения и значительно превосходит исходную модель. Используя ансамбль батч-нормализованных сетей, мы улучшаем лучший опубликованный результат классификации ImageNet: достигая 4,9% ошибки валидации top-5 (и 4,8% ошибки теста), превышая точность человеческих оценщиков."


## 5. Выводы

- **Лучшая модель:** Cohere  
- **Почему:** Она демонстрирует наиболее естественный стиль перевода, реже всех ошибается в переводе терминологии.  
- **Что удивило:** Все модели очень хорошо передают смысл текста, несмотря на ошибки в грамматике или терминологии. Groq и Cohere иногда выдают неожиданные символы (например, иероглифы), но это не мешает пониманию.  

